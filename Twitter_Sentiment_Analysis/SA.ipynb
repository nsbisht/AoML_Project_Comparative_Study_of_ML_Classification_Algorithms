{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application 4: Twitter Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets, preprocessing \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import *\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # for regular expressions \n",
    "import nltk # for text manipulation \n",
    "from nltk.stem.porter import * \n",
    "from wordcloud import WordCloud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[tweets['label'] == 0].head(10) #non racist/sexist tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[tweets['label'] == 1].head(10) #racist/sexist tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_tweets = tweets['tweet'].str.len() \n",
    "plt.hist(length_tweets, bins=20, label=\"tweets\") \n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)\n",
    "    return input_txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing Twitter Handles (@user)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['tidy_tweet'] = np.vectorize(remove_pattern)(tweets['tweet'],\"@[\\w]*\") \n",
    "tweets.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing Punctuations, Numbers, and Special Characters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['tidy_tweet'] = tweets['tidy_tweet'].str.replace('[^a-zA-Z# ]', \" \", regex=True) \n",
    "tweets.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing Short Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['tidy_tweet'] = tweets['tidy_tweet'].apply(lambda x:' '.join([w for w in x.split() if len(w)>3]))\n",
    "tweets.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Text Normalization**\n",
    "\n",
    "Tokenize --> Normalize (using nltk’s PorterStemmer() function)\n",
    "\n",
    "Tokens are individual terms or words.\n",
    "\n",
    "Tokenization is the process of splitting a string of text into tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_tweet = tweets['tidy_tweet'].apply(lambda x: x.split()) #tokenizing \n",
    "tokenized_tweet.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer() \n",
    "tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
    "tokenized_tweet.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join tokens back using nltk’s MosesDetokenizer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(tokenized_tweet)):\n",
    "    detokenized_tweet[i] = ' '.join(tokenized_tweet[i]) \n",
    "tweets['tidy_tweet'] = detokenized_tweet\n",
    "tokenized_tweet.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the common words used in the tweets: WordCloud**\n",
    "\n",
    "Most frequent words appear in large size and the less\n",
    "frequent words appear in smaller sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = ' '.join([text for text in tweets['tidy_tweet']]) \n",
    " \n",
    "wordcloud = WordCloud(width=800, \n",
    "                      height=500,\n",
    "                      random_state=21, \n",
    "                      max_font_size=110).generate(all_words) \n",
    "\n",
    "plt.figure(figsize=(10, 7)) \n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see most of the words are positive or neutral. Words like love, great, friend, life are the most frequent ones. It doesn’t give us any idea about the words associated with the racist/sexist tweets. Hence, we will plot separate wordclouds for both the classes (racist/sexist or not) in our tweets data.\n",
    "\n",
    "Words in non racist/sexist tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_words =' '.join([text for text in tweets['tidy_tweet'][tweets['label'] == 0]]) \n",
    "\n",
    "wordcloud = WordCloud(width=800, \n",
    "                      height=500, \n",
    "                      random_state=21,\n",
    "                      max_font_size=110).generate(normal_words) \n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(wordcloud) \n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Racist/Sexist Tweets\n",
    "negative_words = ' '.join([text for text in tweets['tidy_tweet'][tweets['label'] == 1]])\n",
    "\n",
    "wordcloud = WordCloud(width=800, \n",
    "                      height=500,\n",
    "                      random_state=21, \n",
    "                      max_font_size=110).generate(negative_words)\n",
    "\n",
    "plt.figure(figsize=(10, 7)) \n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the impact of Hashtags on tweets sentiment**\n",
    "\n",
    "Hashtags in twitter are synonymous with the ongoing trends on twitter at any particular point in time. We should try to check whether these hashtags add any value to our sentiment analysis\n",
    "task, i.e., they help in distinguishing tweets into the different sentiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to collect hashtags \n",
    "def hashtag_extract(x): \n",
    "    hashtags = [] \n",
    "    # Loop over the words in the tweet \n",
    "    for i in x: \n",
    "        ht = re.findall(r\"#(\\w+)\", i) \n",
    "        hashtags.append(ht) \n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting hashtags from non racist/sexist tweets \n",
    "HT_regular = hashtag_extract(tweets['tidy_tweet'][tweets['label'] == 0]) \n",
    "print(HT_regular[0:5])\n",
    "\n",
    "# extracting hashtags from racist/sexist tweets \n",
    "HT_negative = hashtag_extract(tweets['tidy_tweet'][tweets['label'] == 1]) \n",
    "print(HT_negative[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unnesting list \n",
    "HT_regular = sum(HT_regular, []) \n",
    "print(HT_regular[0:5])\n",
    "\n",
    "HT_negative = sum(HT_negative, [])\n",
    "print(HT_negative[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non-Racist/Sexist Tweets\n",
    "frequency_distribution = nltk.FreqDist(HT_regular) \n",
    "df = pd.DataFrame({'Hashtag': list(frequency_distribution.keys()), 'Count': list(frequency_distribution.values())}) \n",
    "\n",
    "# selecting top 20 most frequent hashtags \n",
    "df = df.nlargest(columns=\"Count\", n = 20) \n",
    "\n",
    "plt.figure(figsize=(16,5)) \n",
    "ax = sns.barplot(data=df, x= \"Hashtag\", y = \"Count\") \n",
    "ax.set(ylabel = 'Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these hashtags are positive and it makes sense. We expect negative terms in the plot of\n",
    "the second list.\n",
    "\n",
    "**Checking the most frequent hashtags appearing in the racist/sexist tweets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Racist/Sexist Tweets\n",
    "b = nltk.FreqDist(HT_negative) \n",
    "e = pd.DataFrame({'Hashtag': list(b.keys()),'Count': list(b.values())}) \n",
    "\n",
    "# selecting top 20 most frequent hashtags \n",
    "e = e.nlargest(columns=\"Count\", n = 20) \n",
    "plt.figure(figsize=(16,5)) \n",
    "ax = sns.barplot(data=e, x= \"Hashtag\", y = \"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TF-IDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, \n",
    "                                   min_df=2, \n",
    "                                   max_features=1000, \n",
    "                                   stop_words='english') \n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(tweets['tidy_tweet'])\n",
    "vocab = tfidf_vectorizer.vocabulary_\n",
    "tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(tfidf, tweets['label'], random_state=42, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(tfidf)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "classifiers = []\n",
    "\n",
    "lr_classifier = LogisticRegression()#\n",
    "classifiers.append(lr_classifier)\n",
    "lda_classifier = LinearDiscriminantAnalysis()\n",
    "classifiers.append(lda_classifier)\n",
    "svc_classifier = SVC(probability=True)#\n",
    "classifiers.append(svc_classifier)\n",
    "kn_classifier = KNeighborsClassifier()#\n",
    "classifiers.append(kn_classifier)\n",
    "gnb_classifier = GaussianNB() #\n",
    "classifiers.append(gnb_classifier)\n",
    "dt_classifier = DecisionTreeClassifier(max_depth = 10) #\n",
    "classifiers.append(dt_classifier)\n",
    "rf_classifier = RandomForestClassifier()#\n",
    "classifiers.append(rf_classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_classifier = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_classifier.fit(X_train.toarray(), Y_train)\n",
    "y_pred = gnb_classifier.predict(X_test.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_classifier = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_classifier.fit(X_train, Y_train)\n",
    "y_pred = mnb_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(Y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_classifier.fit(X_train, Y_train)\n",
    "prediction = lr_classifier.predict(X_test) \n",
    "print(metrics.accuracy_score(Y_test, prediction))\n",
    "print(metrics.precision_score(Y_test, prediction))\n",
    "print(metrics.recall_score(Y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_classifier.fit(X_train.toarray(), Y_train)\n",
    "prediction = lda_classifier.predict(X_test.toarray()) \n",
    "print(metrics.accuracy_score(Y_test, prediction))\n",
    "print(metrics.precision_score(Y_test, prediction))\n",
    "print(metrics.recall_score(Y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM - Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_classifier.fit(X_train, Y_train)\n",
    "prediction = svc_classifier.predict(X_test) \n",
    "print(metrics.accuracy_score(Y_test, prediction))\n",
    "print(metrics.precision_score(Y_test, prediction))\n",
    "print(metrics.recall_score(Y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-nearest neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kn_classifier.fit(X_train, Y_train)\n",
    "prediction = kn_classifier.predict(X_test) \n",
    "print(metrics.accuracy_score(Y_test, prediction))\n",
    "print(metrics.precision_score(Y_test, prediction))\n",
    "print(metrics.recall_score(Y_test, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_classifier.fit(X_train, Y_train)\n",
    "prediction = dt_classifier.predict(X_test) \n",
    "print(metrics.accuracy_score(Y_test, prediction))\n",
    "print(metrics.precision_score(Y_test, prediction))\n",
    "print(metrics.recall_score(Y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,20))\n",
    "_ = plot_tree(dt_classifier, \n",
    "              filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(\"decision_tree.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier.fit(X_train, Y_train)\n",
    "prediction = rf_classifier.predict(X_test) \n",
    "print(metrics.accuracy_score(Y_test, prediction))\n",
    "print(metrics.precision_score(Y_test, prediction))\n",
    "print(metrics.recall_score(Y_test, prediction))\n",
    "\n",
    "fig = plt.figure(figsize=(25,10))\n",
    "plot_tree(rf_classifier.estimators_[0], \n",
    "                  max_depth = 5,\n",
    "                  rounded = True, \n",
    "                  precision = 2,\n",
    "                  filled = True,\n",
    "                  )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,10))\n",
    "plot_tree(rf_classifier.estimators_[1], \n",
    "                  max_depth = 5,\n",
    "                  rounded = True, \n",
    "                  precision = 2,\n",
    "                  filled = True,\n",
    "                  )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = widgets.Text(description=\"tweet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithm = widgets.Dropdown(\n",
    "    options = [('Logistic Regression', 'LR'), \n",
    "               ('Linear Discriminant Analysis ', 'LDA'), \n",
    "               ('Support Vector Machines', 'SVM'),\n",
    "               ('K-Nearest Neighbors', 'KN'),\n",
    "               ('Multinomial Naive Bayes', 'MNB'),\n",
    "               ('Decision Trees', 'DT'),\n",
    "               ('Random Forest', 'RF'),\n",
    "              ],\n",
    "    disabled = False,\n",
    ")\n",
    "\n",
    "print('Select Algorithm')\n",
    "display(algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = widgets.Output()\n",
    "\n",
    "button_predict = widgets.Button(description=\"Predict\")\n",
    "\n",
    "def on_button_predict_clicked(b):\n",
    "    \n",
    "    input_data = {}\n",
    "    input_data['tweet'] = tweet.value\n",
    "    \n",
    "    user_input = pd.DataFrame(input_data, columns = ['tweet'], index=[0])\n",
    "    #print(user_input)\n",
    "    user_input['tidy_tweet'] = np.vectorize(remove_pattern)(user_input['tweet'],\"@[\\w]*\") \n",
    "    user_input['tidy_tweet'] = user_input['tidy_tweet'].str.replace('[^a-zA-Z# ]', \" \", regex=True) \n",
    "    user_input['tidy_tweet'] = user_input['tidy_tweet'].apply(lambda x:' '.join([w for w in x.split() if len(w)>3]))\n",
    "    tokenized_tweet = user_input['tidy_tweet'].apply(lambda x: x.split()) #tokenizing \n",
    "    tokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\n",
    "    detokenized_tweet = ' '.join(tokenized_tweet[0]) \n",
    "    user_input['tidy_tweet'] = detokenized_tweet\n",
    "    vectorizer = TfidfVectorizer(max_df=0.90, \n",
    "                                   min_df=2, \n",
    "                                   max_features=1000, \n",
    "                                   stop_words='english', vocabulary=vocab) \n",
    "    tf_idf = vectorizer.fit_transform(tweets['tidy_tweet']) \n",
    "    \n",
    "    selected_algorithm = algorithm.value\n",
    "    \n",
    "    if selected_algorithm == 'LR':\n",
    "        classifier = lr_classifier\n",
    "    elif selected_algorithm == 'LDA':\n",
    "        classifier = lda_classifier\n",
    "    elif selected_algorithm == 'SVM':\n",
    "        classifier = svc_classifier        \n",
    "    elif selected_algorithm == 'KN':\n",
    "        classifier = kn_classifier\n",
    "    elif selected_algorithm == 'MNB':\n",
    "        classifier = mnb_classifier\n",
    "    elif selected_algorithm == 'DT':\n",
    "        classifier = dt_classifier\n",
    "    elif selected_algorithm == 'RF':\n",
    "        classifier = rf_classifier\n",
    "        \n",
    "    with prediction:\n",
    "        clear_output(True)\n",
    "        print(f'Selected Algorithm = {selected_algorithm}')\n",
    "        print(classifier.predict(tf_idf)[0])\n",
    "        \n",
    "button_predict.on_click(on_button_predict_clicked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(button_predict)\n",
    "display(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
